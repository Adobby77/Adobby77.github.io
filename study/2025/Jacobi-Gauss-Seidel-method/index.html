<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jacobi &amp; Gauss-Seidel Method | Woojin Shin </title> <meta name="author" content="Woojin Shin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adobby77.github.io/study/2025/Jacobi-Gauss-Seidel-method/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link rel="stylesheet" href="/assets/css/obsidian-compat.css?v=05ae35f9ce652dbe8f02ec6f984e0a75"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Woojin Shin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/study/index.html">Study </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Jacobi &amp; Gauss-Seidel Method</h1> <p class="post-meta"> Created on September 13, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> optimization   <i class="fa-solid fa-hashtag fa-sm"></i> numerical-analysis   <i class="fa-solid fa-hashtag fa-sm"></i> linear-algebra   ·   <i class="fa-solid fa-tag fa-sm"></i> study </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>[!TIP] Download PDF</p> <p>For a cleaner view, please refer to the attached PDF.</p> <div style="margin-top: 10px; display: flex; align-items: center; gap: 15px;"> <a href="/assets/pdf/Jacobi%20&amp;%20Gauss-Seidel%20method.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"> <i class="fas fa-file-pdf"></i> Download PDF </a> <span style="font-size: 0.85rem; color: #888;"> <i class="fas fa-history"></i> Last updated: Sep. 14. 2025. </span> </div> </blockquote> <h2 id="0-introduction">0. Introduction</h2> <p>Jacobi method and Gauss-Seidel method are <strong>classical iterative algorithms to determine the approximate solutions of a large-scale system of linear equations.</strong> They are particularly useful when finding a direct solution by Gaussian elimination is impractical or computationally expensive. They computes the iterative solution from the initial guess until it converges to the exact solution within a desired tolerance.</p> <p>The fundamental difference between them is the way they utilize information during the process. <strong>The Jacobi method computes a new set of values for the solution vector using only the values from the previous iteration.</strong> In contrast, <strong>the Gauss-Seidel method immediately use the most recently computed values from the current iteration in subsequent calculations within that same step.</strong> While the Jacobi method is more convenient to formulate the problem in a parallel manner, the Gauss-Seidel method converges faster than the Jacobi method.</p> <div style="height: 0.1em;"></div> <hr> <h2 id="1-problem-formulation">1. Problem formulation</h2> <blockquote> <p>[!note] Problem formulation</p> <p><strong>Solve</strong> $Ax=b$ where $A\in\mathbb{R}^{n\times n}$, $x, b\in\mathbb{R}^n$</p> <div class="kdmath">$$ A=\begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{n2}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \end{bmatrix} ,\quad b=\begin{bmatrix} b_1\\ b_2\\ \vdots\\ b_n \end{bmatrix} \quad\text{and}\quad x=\begin{bmatrix} x_1\\ x_2\\ \vdots\\ x_n \end{bmatrix} $$</div> <p>From a different point-of view, solving $Ax=b$ is equivalent to minimizing $\lVert Ax-b \rVert$. We can generalize this algorithm to the general optimization problem.</p> </blockquote> <hr> <h2 id="2-algorithms">2. Algorithms</h2> <div style="height: 0.1em;"></div> <h3 id="21-the-jacobi-method">2.1. The Jacobi method</h3> <p>Separate $A$ into 3 parts, i.e. $A=D+L+U$ where $D$ is diagonal, $L$ is strictly lower triangular, $U$ is strictly upper triangular parts of $A$.</p> <p>Then, we have $(D+L+U)x=b$ and hence</p> <blockquote> <p>[!algorithm] Jacobi Method</p> <div class="kdmath">$$ \begin{aligned} Dx + (L+U)x = b&amp; &amp;&amp; \text{(given)}\\[2pt] Dx = b - (L+U)x&amp; &amp;&amp; \text{(move $(L+U)x$)}\\[2pt] x = D^{-1}\!\bigl(b-(L+U)x\bigr)&amp; &amp;&amp; \text{(left-multiply $D^{-1}$)} \end{aligned} $$</div> </blockquote> <p>So, from our initial guess $x^{(0)}$, we can update $x$ using such an iterative algorithm.</p> <div class="kdmath">$$ x^{(k+1)}=D^{-1}(b-(L+U)x^{(k)}) $$</div> <p>The decomposed matrices are</p> <div class="kdmath">$$ D=\begin{bmatrix} a_{11} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; a_{22} &amp; \cdots &amp; 0\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ 0 &amp; 0&amp; \cdots &amp; a_{nn} \end{bmatrix},\quad U=\begin{bmatrix} 0 &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ 0 &amp; 0 &amp; \cdots &amp; a_{2n}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ 0 &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix},\quad L=\begin{bmatrix} 0 &amp; 0 &amp; \cdots &amp; 0 \\ a_{21} &amp; 0 &amp; \cdots &amp; 0\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; 0 \end{bmatrix} $$</div> <p>and hence</p> <div class="kdmath">$$ D^{-1}=\begin{bmatrix} \frac{1}{a_{11}} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \frac{1}{a_{22}} &amp; \cdots &amp; 0\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ 0 &amp; 0&amp; \cdots &amp; \frac{1}{a_{nn}} \end{bmatrix},\quad L+U=\begin{bmatrix} 0 &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; 0 &amp; \cdots &amp; a_{2n}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; 0 \end{bmatrix},\quad $$</div> <p>Thus, we can rewrite the update equation in terms of elements like:</p> <div class="kdmath">$$ x_i^{(k+1)}=\frac{1}{a_{ii}}\left(b-\sum_{j\neq i}a_{ij}x_j^{(k)}\right), \quad\forall i. $$</div> <div style="height: 0.1em;"></div> <hr> <h3 id="22-the-gauss-seidel-method">2.2. The Gauss-Seidel method</h3> <p>Same to the Jacobi method, separate $A$ into 3 parts, i.e. $A=D+L+U$ where $D$ is diagonal, $L$ is strictly lower triangular, $U$ is strictly upper triangular parts of $A$.</p> <p>Then, we have $(D+L+U)x=b$ and hence</p> <blockquote> <p>[!algorithm] Gauss-Seidel Method</p> <div class="kdmath">$$ \begin{aligned} (D + L)x+Ux = b&amp; &amp;&amp; \text{(given)}\\[2pt] (D+L)x = b - Ux&amp; &amp;&amp; \text{(move $Ux$)}\\[2pt] x = (D+L)^{-1}(b-Ux)&amp; &amp;&amp; \text{(left-multiply $(D+L)^{-1}$)} \end{aligned} $$</div> </blockquote> <p>So, from our initial guess $x^{(0)}$, we can update $x$ using such an iterative algorithm.</p> <div class="kdmath">$$ x^{(k+1)}=(D+L)^{-1}(b-Ux^{(k)}) $$</div> <p>The decomposed matrices are</p> <div class="kdmath">$$ D+L=\begin{bmatrix} a_{11} &amp; 0 &amp; \cdots &amp; 0 \\ a_{21} &amp; a_{22} &amp; \cdots &amp; 0\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \end{bmatrix},\quad U=\begin{bmatrix} 0 &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ 0 &amp; 0 &amp; \cdots &amp; a_{2n}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ 0 &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix} $$</div> <p>and hence the element-based formula is</p> <div class="kdmath">$$ x_i^{(k+1)}=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\sum_{j=i+1}^na_{ij}x_j^{(k)}\right) $$</div> <hr> <p>As we can see, both algorithms require <strong>central factors</strong> and hence not applicable to the fully deectralized optimization problems.</p> <div style="height: 0.1em;"></div> <h2 id="3-convergence">3. Convergence</h2> <h3 id="31-preliminaries">3.1. Preliminaries</h3> <h5 id="def-31-spectral-radius">Def 3.1) Spectral radius</h5> <blockquote> <p>For a square matrix $A\in\mathbb{C}^{n\times n}$, the spectral radius of $A$ is the maximum value of the absolute values of its eigenvalues and denoted by $\rho(A)$.</p> <div class="kdmath">$$ \rho(A)=\max\{|\lambda_1|,\cdots,|\lambda_n|\} $$</div> </blockquote> <div style="height: 0.1em;"></div> <h5 id="lem-32-relationship-between-norm-and-spectral-radius">Lem 3.2) Relationship between Norm and Spectral Radius</h5> <blockquote> <p>For any matrix $T\in\mathbb{C}^{n \times n}$ and any $\epsilon &gt; 0$, there exists a subordinate matrix norm $\lnm\cdot\rnm$ such that:</p> <div class="kdmath">$$ \left\lVert T\right\rVert \le \rho(T) + \epsilon $$</div> </blockquote> <div style="height: 0.1em;"></div> <h5 id="def-33-diagonally-dominant-matrix">Def 3.3) Diagonally Dominant matrix</h5> <blockquote> <p>A square matrix $A\in\mathbb{C}^{n\times n}$ is said to be diagonally dominant if for every row of matrix, the magnitude of the diagonal entry in a row is greater than or equal to the sum of magnitudes of all the off-diagonal entries in that row.</p> <div class="kdmath">$$ |a_{ii}|\geq\sum_{j\neq i}|a_{ij}|, \quad\forall i $$</div> </blockquote> <div style="height: 0.1em;"></div> <h5 id="thm-34-gershgorins-circle-theorem">Thm 3.4) Gershgorin’s Circle Theorem</h5> <blockquote> <p>Let A be a square matrix $A\in\mathbb{C}^{n\times n}$ with entries $a_{ij}$. For $i=1,\cdots,n$, let $R_i$ be the sum of the absolute values of the off-diagoonal entries in the $i^\textrm{th}$ row, i.e. $R_i=\sum_{j\neq i}|a_{ij}|$. Then, <strong>a closed disc centered at $a_{ii}$ with radius $R_i$ in the complex plane is called a Gershgorin disc</strong>, which is denoted by $D(a_{ii},R_i)\subset\mathbb{C}$ .</p> <p>Then, <strong>every eigenvalue of $A$ lies within at least one of the Gershgorin discs $D(a_{ii}, R_i)$.</strong> In other words, <strong>every eigenvalue of $A$ is in the union of the Gershgorin discs, i.e. $\lambda(A)\in\bigcup_iD(a_ii,R_i)$.</strong></p> </blockquote> <div style="height: 0.1em;"></div> <p><strong>Proof.</strong> Let $\lambda$ be the eigenvalue of $A$ and $x$ be the eigenvector corresponds to $\lambda$, i.e. $Ax=\lambda x$.</p> <p>Let $x_{j^*}$ be the element whose magnitude is the largest among the elements of $x$, i.e.</p> <div class="kdmath">$$ |x_{i}|=\max_{j}|x_j| $$</div> <p>Since $x\neq 0,\; \left\vert x_i\right\vert&gt;0$.</p> <p>Now, from the equation $Ax=\lambda x$, observe the $i^{\textrm{th}}$ row only.</p> <div class="kdmath">$$ \lambda x_i=\sum_{j}a_{ij}\frac{x_j}{x_i} $$</div> <p>and hence</p> <div class="kdmath">$$ \lambda-a_{ii}=\sum_{j\neq i}a_{ij}\frac{x_j}{x_i} $$</div> <p>Take absolute value</p> <div class="kdmath">$$ |\lambda-a_{ii}|=\left|\sum_{j\neq i}a_{ij}\frac{x_j}{x_i}\right| $$</div> <p>By triangle inequality,</p> <div class="kdmath">$$ \left|\sum_{j\neq i}a_{ij}\frac{x_j}{x_i}\right|=\sum_{j\neq i}\left|a_{ij}\frac{x_j}{x_i}\right| $$</div> <p>and from the fact that $\left\vert x_i\right\vert\geq\left\vert x_j\right\vert$ ,</p> <div class="kdmath">$$ \sum_{j\neq i}\left|a_{ij}\frac{x_j}{x_i}\right|=\sum_{j\neq i}\left|a_{ij}\right|\left|\frac{x_j}{x_i}\right|\leq\sum_{j\neq i}\left|a_{ij}\right|=R_i $$</div> <p>Thus, we can conclude that $\left\vert\lambda-a_{ii}\right\vert\leq R_i$ and hence, proved.<span style="float: right;">$\square$</span></p> <div style="height: 0.1em;"></div> <h5 id="cor-35-nonsingularity-of-strict-diagonally-dominant-matrix">Cor 3.5) Nonsingularity of Strict Diagonally dominant matrix</h5> <blockquote> <p>A strictly diagonally dominant matrix (or an irreducibly diagonally dominant matrix is nonsingular.</p> </blockquote> <div style="height: 0.1em;"></div> <h3 id="32-convergence-theorems">3.2. Convergence Theorems</h3> <p>Both iterative methods are said to converge when $\lnm x^{(k+1)}-x^{(k)}\rnm&lt;\varepsilon$ for any $\varepsilon&gt;0$. There are some conditions to guarantee the theoretical convergence of Jacobi method.</p> <h5 id="thm-36-convergence-for-strictly-diagonally-dominant-matrix">Thm 3.6) Convergence for strictly diagonally dominant matrix</h5> <blockquote> <p>For a system of linear equations $Ax=b$, both the Jacobi and the Gauss-Seidel method converges to the unique solution if $A$ is strictly diagoally dominant. (Sufficient condition)</p> </blockquote> <div style="height: 0.1em;"></div> <p><strong>Proof for the Jacobi method</strong> Since $A$ is strictly diagonally dominant, $A$ is nonsingular and hence invertible. This implies there exists an exact solution $x^*=A^{-1}b$.</p> <p>From the element-based Jacobi method,</p> <div class="kdmath">$$ x_i^{(k+1)}=\frac{1}{a_{ii}}\left(b_i-\sum_{j\neq i}a_{ij}x_j^{(k)}\right), \quad\forall i, $$</div> <p>we can denote the exact solution by dropping the iteration index $k$.</p> <div class="kdmath">$$ x_i^*=\frac{1}{a_{ii}}\left(b_i-\sum_{j\neq i}a_{ij}x_j^*\right), \quad\forall i, $$</div> <p><strong>Claim: Error Converges to 0</strong></p> <p>In this setting, we can define an error at iteration $k$ by taking an absolute value of the difference between the value at iteration $k$ and the exact solution.</p> <div class="kdmath">$$ e_i^{(k+1)}=x_i^{(k+1)}-x_i^*=\frac{1}{a_{ii}}\sum_{j\neq i}a_{ij}\left(x_j^{(k)}-x_j^*\right)=\frac{1}{a_{ii}}\sum_{j\neq i}a_{ij}e_j^{(k)} $$</div> <p>Taking absolute value and apply the triangle inequality.</p> <div class="kdmath">$$ \left\vert e_i^{(k+1)}\right\vert=\left\vert\frac{1}{a_{ii}}\sum_{j\neq i}a_{ij}e_j^{(k)}\right\vert\leq\left\vert\frac{1}{a_{ii}}\right\vert\sum_{j\neq i}\left\vert a_{ij}e_j^{(k)}\right\vert=\sum_{j\neq i}\left\vert\frac{a_{ij}}{a_{ii}}\right\vert\left\vert e_j^{(k)}\right\vert $$</div> <p>Since $A$ is strictly diagonally dominant, $C_i=\sum_{j\neq i}\left\vert\frac{a_{ij}}{a_{ii}}\right\vert&lt;1$ and hence we have</p> <div class="kdmath">$$ \left|e_i^{(k+1)}\right|\leq C_i\left|e_j^{(k)}\right| $$</div> <p>Then, by the definition of $l_\infty$ norm,</p> <div class="kdmath">$$ \left\lVert e^{(k+1)}\right\rVert_\infty \leq C\left\lVert e^{(k)}\right\rVert_\infty\quad\text{where}\quad C&lt;1 $$</div> <p>Thus, we can conclude that the error converges to zero.<span style="float: right;">$\square$</span></p> <hr> <p><strong>Proof for the Gauss-Seidel method</strong> The proof for the Gauss-Seidel method utilizes same idea to that of the Jacobi method.</p> <p>Since $A$ is strictly diagonally dominant, $A$ is nonsingular and hence invertible. This implies there exists an exact solution $x^*=A^{-1}b$.</p> <p>From the element-based Gauss-Seidel method,</p> <p><span class="kdmath">$x_i^{(k+1)}=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\sum_{j=i+1}^{n}a_{ij}x_j^{(k)}\right), \quad\forall i,$</span> we can denote the exact solution by dropping the iteration index $k$.</p> <div class="kdmath">$$ x_i^*=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^{i-1}a_{ij}x_j^*-\sum_{j=i+1}^{n}a_{ij}x_j^*\right), \quad\forall i, $$</div> <p><strong>Claim: Error Converges to 0</strong></p> <p>In this setting, we can define an error at iteration $k$ by taking a difference between the value at iteration $k$ and the exact solution, $e_i^{(k)} = x_i^{(k)}-x_i^*$. Subtracting the second equation from the first yields the error propagation formula:</p> <div class="kdmath">$$ e_i^{(k+1)}=x_i^{(k+1)}-x_i^*=-\frac{1}{a_{ii}}\left(\sum_{j=1}^{i-1}a_{ij}e_j^{(k+1)}+\sum_{j=i+1}^{n}a_{ij}e_j^{(k)}\right) $$</div> <p>Taking the absolute value and applying the triangle inequality, we get:</p> <div class="kdmath">$$ \left|e_i^{(k+1)}\right|\leq\frac{1}{\left|a_{ii}\right|}\left(\sum_{j=1}^{i-1}\left|a_{ij}\right|\left|e_j^{(k+1)}\right|+\sum_{j=i+1}^{n}\left|a_{ij}\right|\left|e_j^{(k)}\right|\right) $$</div> <p>Let $\left\vert e^{(k+1)}\right\vert_\infty = \max_i\left\vert e_i^{(k+1)}\right\vert = \left\vert e_p^{(k+1)}\right\vert$ for some index $p$. For this specific component $p$, the inequality becomes:</p> <div class="kdmath">$$ \|e^{(k+1)}\|_\infty \leq \frac{1}{|a_{pp}|} \left( \left(\sum_{j=1}^{p-1}|a_{pj}|\right) \|e^{(k+1)}\|_\infty + \left(\sum_{j=p+1}^{n}|a_{pj}|\right) \|e^{(k)}\|_\infty \right) $$</div> <p>We can now solve for $|e^{(k+1)}|_\infty$:</p> <div class="kdmath">$$ \|e^{(k+1)}\|_\infty \left( 1 - \frac{\sum_{j=1}^{p-1}|a_{pj}|}{|a_{pp}|} \right) \le \left( \frac{\sum_{j=p+1}^{n}|a_{pj}|}{|a_{pp}|} \right) \|e^{(k)}\|_\infty $$</div> <div class="kdmath">$$ \|e^{(k+1)}\|_\infty \le \left( \frac{\sum_{j=p+1}^{n}|a_{pj}|}{|a_{pp}| - \sum_{j=1}^{p-1}|a_{pj}|} \right) \|e^{(k)}\|_\infty $$</div> <p>Since $A$ is strictly diagonally dominant, we know that $\left\vert a_{pp}\right\vert &gt; \sum_{j \neq p}\left\vert a_{pj}\right\vert = \sum_{j=1}^{p-1}\left\vert a_{pj}\right\vert + \sum_{j=p+1}^{n}\left\vert a_{pj}\right\vert$.</p> <p>This implies $\left\vert a_{pp}\right\vert - \sum_{j=1}^{p-1}\left\vert a_{pj}\right\vert &gt; \sum_{j=p+1}^{n}\left\vert a_{pj}\right\vert$. Let us define the constant as $C_p = \frac{\sum_{j=p+1}^{n}\left\vert a_{pj}\right\vert}{\left\vert a_{pp}\right\vert - \sum_{j=1}^{p-1}\left\vert a_{pj}\right\vert}$, it follows that $C_p &lt; 1$.</p> <p>Then, by the definition of $l_\infty$ norm,</p> <div class="kdmath">$$ \left\lVert e^{(k+1)}\right\rVert_\infty \leq C\left\lVert e^{(k)}\right\rVert_\infty\quad\text{where}\quad C=\max_p C_p &lt; 1 $$</div> <p>Thus, we can conclude that the error converges to zero.<span style="float: right;">$\square$</span></p> <hr> <h5 id="thm-37-convergence-of-general-itermative-method">Thm 3.7) Convergence of general itermative method</h5> <blockquote> <p>An iterative method of the form $x^{(k+1)} = Tx^{(k)} + c$ converges to the unique solution of $x=Tx+c$ for any initial guess vector $x^{(0)}$ if and only if the spectral radius $\rho(T)$ of the iteration matrix $T$ is less than 1.</p> <div class="kdmath">$$ \rho(T) &lt; 1 $$</div> <p>In this setting, $T_J=D^{-1}(L+U)$ and $T_G=(D+L)^{-1}U$ for the Jacobi and Gauss-Seidel methods, respectively.</p> </blockquote> <div style="height: 0.1em;"></div> <p><strong>Proof.</strong> The error vector $e^{(k)} = x^{(k)} - x^*$ follows the recurrence $e^{(k+1)} = T e^{(k)}$, which implies $e^{(k)} = T^k e^{(0)}$. The method converges if and only if $\lim_{k \to \infty} e^{(k)} = 0$ for any initial error $e^{(0)}$. This is equivalent to the condition that $\lim_{k \to \infty} T^k = 0$ (the zero matrix).</p> <p><strong>(Necessity: $\lim_{k \to \infty} T^k = 0 \implies \rho(T) &lt; 1$)</strong> Let $\lambda$ be any eigenvalue of $T$ with a corresponding eigenvector $v \neq 0$. By definition, $Tv = \lambda v$.</p> <p>Applying $T$ repeatedly, we get $T^k v = \lambda^k v$. Taking the limit as $k \to \infty$, and using our assumption that $T^k \to 0$:</p> <div class="kdmath">$$ \lim_{k \to \infty} \left(T^k v\right) = \left(\lim_{k \to \infty} T^k\right) v = 0 \cdot v = 0 $$</div> <p>This means we must have $\lim_{k \to \infty} (\lambda^k v) = 0$. Since $v$ is a non-zero vector, the scalar sequence $\lambda^k$ must converge to 0. This is only possible if $\left\vert\lambda\right\vert &lt; 1$. Since this must hold for every eigenvalue of $T$, it must hold for the one with the largest magnitude. Thus, $\rho(T) &lt; 1$.</p> <p><strong>(Sufficiency: $\rho(T) &lt; 1 \implies \lim_{k \to \infty} T^k = 0$)</strong> This direction of the proof relies on a lemma of the relationship between norm and spectral radius.</p> <p>By our assumption, $\rho(T) &lt; 1$. We can choose an $\epsilon &gt; 0$ that is small enough such that $\rho(T) + \epsilon &lt; 1$. For instance, we can choose $\epsilon = (1 - \rho(T))/2$.</p> <p>According to the lemma, there exists a matrix norm for which $\left\lVert T\right\rVert \le \rho(T) + \epsilon = C$, where $C$ is a constant less than 1.</p> <p>Using the submultiplicative property of matrix norms $\left\lVert A^k\right\rVert \le \left\lVert A\right\rVert^k$, we have:</p> <div class="kdmath">$$ \left\lVert T^k\right\rVert \le \left\lVert T\right\rVert^k \le C^k $$</div> <p>As $k \to \infty$, since $C &lt; 1$, we have $C^k \to 0$. This implies that $\lim_{k \to \infty} \left\lVert T^k\right\rVert = 0$. If the norm of a matrix converges to zero, the matrix itself must converge to the zero matrix. Therefore, $\lim_{k \to \infty} T^k = 0$.<span style="float: right;">$\square$</span></p> <div style="height: 0.1em;"></div> <hr> <h5 id="thm-38-convergence-of-the-gauss-seidel-method">Thm 3.8) Convergence of the Gauss-Seidel method</h5> <blockquote> <p>If $A^\mathsf{T}=A\succeq0$, then the Gauss-Seidel iterates converge to $x^*=A^{-1}b$ for any initial guess $x^{(0)}$.</p> </blockquote> <div style="height: 0.1em;"></div> <hr> <h5 id="thm-39-steinrosenberg-theorem-convergence-rate">Thm 3.9) Stein–Rosenberg theorem (Convergence rate)</h5> <blockquote> <p>Let $A=(a_{ij})\in\mathbb{R}^{n\times n}$ and let $\rho(T)$ be the spectral radius of a matrix $T$. Let $T_J, T_G$ be the matrix splitting for the Jacobi method and the Gauss-Seidel method, respectively. If $a_{ij} \leq 0$, for $i \neq j$ and $a_{ii} &gt; 0$, for $i = 1,2,\ldots,n$, then one and only one of the following statements holds (four statements are mutually exclusive):</p> <p>i) $0 \leq \rho(T_G) &lt; \rho(T_J) &lt; 1$<br> ii) $1 &lt; \rho(T_J) &lt; \rho(T_G)$<br> iii) $\rho(T_J) = \rho(T_G) = 0$<br> iv) $\rho(T_J) = \rho(T_G) = 1$</p> </blockquote> <div style="height: 0.1em;"></div> <hr> <h2 id="4-variants">4. Variants</h2> <h3 id="41-method-of-successive-over-relaxation-sor">4.1. Method of Successive over-relaxation (SOR)</h3> <p>The method of successive over-relaxation is a variant of the Gauss-Seidel method for solving a system of linear equations for faster convergence by introducing a relaxation factor.</p> <h4 id="411-algorithm">4.1.1. Algorithm</h4> <p>As with the Gauss-Seidal method, separate $A$ into 3 parts, i.e. $A=D+L+U$ where $D$ is diagonal, $L$ is strictly lower triangular, $U$ is strictly upper triangular parts of $A$.</p> <p>Then, we have $(D+L+U)x=b$ and multiply the relaxation factor $\omega&gt;1$ both sides. <span class="kdmath">$\omega\cdot(D+L+U)x=wb$</span> By moving terms properly, we have</p> <blockquote> <p>[!algorithm] SOR</p> <div class="kdmath">$$ \begin{align} &amp; (D+\omega L)x = \omega b-[\omega U+(\omega-1)D]x\\ &amp; x=(D+\omega L)^{-1}\left(\omega b-[\omega U+(\omega -1)D]x\right) \end{align} $$</div> </blockquote> <p>Thus, the iterative update can be expressed by:</p> <div class="kdmath">$$ x^{(k+1)}=(D+\omega L)^{-1}\left(\omega b-[\omega U+(\omega -1)D]x^{(k)}\right) $$</div> <p>and its element-based expression is</p> <div class="kdmath">$$ x_i^{(k+1)} = (1 - \omega)x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j &lt; i} a_{ij} x_j^{(k+1)} - \sum_{j &gt; i} a_{ij} x_j^{(k)} \right), \quad i = 1,2,\ldots,n. $$</div> <p>and it can be also directly expressed by</p> <div class="kdmath">$$ x^{(k+1)} = (1 - \omega)x^{(k)} + \omega D^{-1} \left(b-Lx^{(k+1)}-Ux^{(k)}\right). $$</div> <p>this expression is more convenient because it doesn’t requires the computation of $(D+\omega L)^{-1}$.</p> <p>If $A$ is symmetric, i.e. $L=U^\mathsf{T}$, such a method is called the Symmetric Successive over-relaxation (SSOR).</p> <div style="height: 0.1em;"></div> <h4 id="412-convergence">4.1.2. Convergence</h4> <h5 id="thm-41-kahan">Thm 4.1) Kahan</h5> <blockquote> <p>If $a_{ii} \neq 0$ for $i = 1, \ldots, n$, then the SOR iteration matrix $T_\omega$ satisfies</p> <p><span class="kdmath">$\rho(T_\omega) \geq |\omega - 1|$</span> Consequently, the SOR iterates converge for every $x^{(0)}$ only if $0 &lt; \omega &lt; 2$.</p> </blockquote> <div style="height: 0.1em;"></div> <h5 id="thm-42-ostrowskireich">Thm 4.2) Ostrowski–Reich</h5> <blockquote> <p>If $A^\mathsf{T}=A\succeq0$ and $0 &lt; \omega &lt; 2$, then the SOR iterates converge to $A^{-1}b$ for every $x^{(0)}$.</p> </blockquote> <div style="height: 0.1em;"></div> <h5 id="thm-43-determining-the-relaxation-factor">Thm 4.3) Determining the relaxation factor</h5> <blockquote> <p>If $A$ is symmetric positive-definite and tridiagonal, then</p> <div class="kdmath">$$ \rho(T_{G}) = \rho(T_J)^2 &lt; 1, $$</div> <p>and the $\omega$ that minimizes $\rho(T_\omega)$ is</p> <div class="kdmath">$$ \omega = \frac{2}{1 + \sqrt{1 - \rho(T_J)^2}} $$</div> <p>For this $\omega$, $\rho(T_\omega) = \omega - 1$.</p> </blockquote> <hr> <h2 id="5-applications">5. Applications</h2> <h3 id="51-2d-laplaces-equation-for-steady-state-analysis">5.1. 2D Laplace’s equation for steady-state analysis</h3> <p>The problem of calculating the steady-state temperature distribution or electrostatic potential on a 2D plane is described by Laplace’s equation:</p> <div class="kdmath">$$ \nabla^2 u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0 $$</div> <p>Approximating this with the finite difference method, the second partial derivative term at each grid point $(i,j)$ is expressed as</p> <div class="kdmath">$$ \begin{align} \frac{\partial^2 u}{\partial x^2} \approx \frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2}\\ \frac{\partial^2 u}{\partial y^2} \approx \frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} \end{align} $$</div> <p>and thus the entire equation is transformed into a simple algebraic equation by substituting them into the original PDE.</p> <div class="kdmath">$$ 4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = 0 $$</div> <p>By setting up this equation for every interior grid point, a large, diagonally dominant matrix is formed where the diagonal element of each row is 4 and the off-diagonal elements are -1 or 0. It can be solved using the Gauss-Seidel method.</p> <div class="kdmath">$$ u_{i,j}^{(k+1)} = \frac{1}{4} \Big( u_{i-1,j}^{(k+1)} + u_{i,j-1}^{(k+1)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k)} \Big) $$</div> <h3 id="52-time-dependent-1d-heat-equation">5.2. Time-dependent 1D Heat Equation</h3> <p>1D heat equation which describes time-dependent phenomena is</p> <div class="kdmath">$$ \frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2} $$</div> <p>Using an implicit method for a stable solution means the time derivative is approximated as</p> <div class="kdmath">$$ \frac{\partial u}{\partial t}\approx\frac{u_i^{n+1} - u_i^n}{\Delta t} $$</div> <p>and the spatial derivative is approximated using the values at the next time step $(n+1)$ as</p> <div class="kdmath">$$ \frac{\partial^2 u}{\partial x^2}\approx\frac{u_{i+1}^{n+1} - 2u_i^{n+1} + u_{i-1}^{n+1}}{h^2} $$</div> <p>By substituting them into the original equation and rearrange it for the unknown $u_i^{n+1}$ at the next time step yields a tridiagonal matrix system, we have</p> <div class="kdmath">$$ -ru_{i-1}^{n+1} + (1+2r)u_i^{n+1} - ru_{i+1}^{n+1} = u_i^n $$</div> <p>where $r = \alpha \Delta t / h^2$. It can also be solved at each time step using the Gauss-Seidel method.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/study/2026/CBF/">Control Barrier Functions</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/study/2025/welcome/">Welcome to My Study Notes</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Woojin Shin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=4c7ba839817bcbd792b494605a85f4db"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> <script src="/assets/js/obsidian-compat.js?v=d8cb3b9cd436ff9007912f2f758b4d85"></script> </body> </html>